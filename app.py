import streamlit as st
import cv2
from ultralytics import YOLO
import numpy as np
from collections import defaultdict
import pandas as pd
from datetime import datetime
import tempfile
import os
import time
from multiprocessing import Process, Queue, Manager
import subprocess

# Page configuration
st.set_page_config(page_title="Vehicle Counter", layout="wide")

# Title with link to Fiverr
st.markdown("""
    <h1 style='text-align: center;'>
        üöó FHWA Vehicle Counter by 
        <a href='https://fiverr.com/naseem733' target='_blank' style='color: #1DBF73; text-decoration: none;'>
            Naseem Awan
        </a>
    </h1>
""", unsafe_allow_html=True)

# Initialize session state for persistence
if 'processed' not in st.session_state:
    st.session_state.processed = False
if 'output_video_path' not in st.session_state:
    st.session_state.output_video_path = None
if 'csv_data' not in st.session_state:
    st.session_state.csv_data = None
if 'results_df' not in st.session_state:
    st.session_state.results_df = None
if 'class_counts' not in st.session_state:
    st.session_state.class_counts = None
if 'total_count' not in st.session_state:
    st.session_state.total_count = 0
if 'processing_time' not in st.session_state:
    st.session_state.processing_time = 0
if 'video_duration' not in st.session_state:
    st.session_state.video_duration = 0

# Sidebar configuration
st.sidebar.header("‚öôÔ∏è Configuration")
confidence = st.sidebar.slider("Detection Confidence", 0.1, 1.0, 0.25, 0.05)
skip_frames = st.sidebar.slider("Skip Frames (Speed)", 1, 10, 4)
line_position = st.sidebar.slider("Counting Line Position", 0.0, 1.0, 0.5, 0.05)

# Parallel processing option
st.sidebar.header("üöÄ Parallel Processing")
enable_parallel = st.sidebar.checkbox("Enable Parallel Processing (>2hr videos)", value=True,
                                     help="Split video into 2 parts and process simultaneously")

# FHWA Vehicle Classes
st.sidebar.header("üìä FHWA Vehicle Classes")
fhwa_classes = {
    1: "Motorcycle",
    2: "Passenger Car",
    3: "Pickup/Van",
    4: "Bus",
    5: "Single Unit Truck (2-axle)",
    6: "Single Unit Truck (3-axle)",
    7: "Single Unit Truck (4+ axle)",
    8: "Single Trailer Truck (3-4 axle)",
    9: "Single Trailer Truck (5-axle)",
    10: "Single Trailer Truck (6+ axle)",
    11: "Multi-Trailer Truck (5 axle)",
    12: "Multi-Trailer Truck (6 axle)",
    13: "Multi-Trailer Truck (7+ axle)"
}

for cls, name in fhwa_classes.items():
    st.sidebar.text(f"Class {cls}: {name}")

# Helper function to format time
def format_time(seconds):
    if seconds < 60:
        return f"{seconds:.1f} seconds"
    elif seconds < 3600:
        minutes = seconds / 60
        return f"{minutes:.1f} minutes"
    else:
        hours = seconds / 3600
        minutes = (seconds % 3600) / 60
        return f"{hours:.1f}h {minutes:.0f}m"

# YOLO to FHWA mapping
def map_to_fhwa(yolo_class, bbox_area):
    """Map YOLO class to FHWA vehicle class"""
    if yolo_class == 3:  # motorcycle
        return 1
    elif yolo_class == 2:  # car
        return 2 if bbox_area < 5000 else 3
    elif yolo_class == 5:  # bus
        return 4
    elif yolo_class == 7:  # truck
        if bbox_area < 8000:
            return 5
        elif bbox_area < 15000:
            return 8
        else:
            return 9
    return 2

# Vehicle tracker
class VehicleTracker:
    def __init__(self, max_disappeared=30, max_distance=100):
        self.next_object_id = 0
        self.objects = {}
        self.disappeared = {}
        self.counted = set()
        self.max_disappeared = max_disappeared
        self.max_distance = max_distance
        
    def register(self, centroid, fhwa_class):
        self.objects[self.next_object_id] = {
            'centroid': centroid,
            'class': fhwa_class,
            'crossed': False
        }
        self.disappeared[self.next_object_id] = 0
        self.next_object_id += 1
        
    def deregister(self, object_id):
        del self.objects[object_id]
        del self.disappeared[object_id]
        
    def update(self, detections, line_y):
        newly_counted = []
        
        if len(detections) == 0:
            for object_id in list(self.disappeared.keys()):
                self.disappeared[object_id] += 1
                if self.disappeared[object_id] > self.max_disappeared:
                    self.deregister(object_id)
            return newly_counted
        
        input_centroids = np.array([d[0] for d in detections])
        input_classes = [d[1] for d in detections]
        
        if len(self.objects) == 0:
            for i, (centroid, fhwa_class) in enumerate(detections):
                self.register(centroid, fhwa_class)
        else:
            object_ids = list(self.objects.keys())
            object_centroids = np.array([self.objects[oid]['centroid'] for oid in object_ids])
            
            D = np.linalg.norm(object_centroids[:, np.newaxis] - input_centroids, axis=2)
            
            rows = D.min(axis=1).argsort()
            cols = D.argmin(axis=1)[rows]
            
            used_rows = set()
            used_cols = set()
            
            for (row, col) in zip(rows, cols):
                if row in used_rows or col in used_cols:
                    continue
                    
                if D[row, col] > self.max_distance:
                    continue
                    
                object_id = object_ids[row]
                old_centroid = self.objects[object_id]['centroid']
                new_centroid = input_centroids[col]
                
                self.objects[object_id]['centroid'] = new_centroid
                self.disappeared[object_id] = 0
                
                if (object_id not in self.counted and 
                    not self.objects[object_id]['crossed'] and
                    old_centroid[1] < line_y <= new_centroid[1]):
                    
                    self.objects[object_id]['crossed'] = True
                    self.counted.add(object_id)
                    newly_counted.append(self.objects[object_id]['class'])
                
                used_rows.add(row)
                used_cols.add(col)
            
            unused_rows = set(range(D.shape[0])) - used_rows
            for row in unused_rows:
                object_id = object_ids[row]
                self.disappeared[object_id] += 1
                if self.disappeared[object_id] > self.max_disappeared:
                    self.deregister(object_id)
            
            unused_cols = set(range(D.shape[1])) - used_cols
            for col in unused_cols:
                self.register(input_centroids[col], input_classes[col])
        
        return newly_counted

# Function to split video into two parts using ffmpeg
def split_video(input_path, output_path1, output_path2):
    """Split video into two equal parts using ffmpeg"""
    try:
        # Get video duration
        cap = cv2.VideoCapture(input_path)
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        duration = total_frames / fps
        cap.release()
        
        mid_point = duration / 2
        
        # Split using ffmpeg (much faster than frame-by-frame)
        # First half
        cmd1 = [
            'ffmpeg', '-i', input_path,
            '-t', str(mid_point),
            '-c', 'copy',
            '-y', output_path1
        ]
        
        # Second half
        cmd2 = [
            'ffmpeg', '-i', input_path,
            '-ss', str(mid_point),
            '-c', 'copy',
            '-y', output_path2
        ]
        
        subprocess.run(cmd1, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        subprocess.run(cmd2, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        
        return True, duration
    except Exception as e:
        st.error(f"Error splitting video: {e}")
        return False, 0

# Process video segment (for parallel processing)
def process_video_segment(video_path, confidence, skip_frames, line_position, 
                         segment_id, result_queue, progress_dict):
    """Process a video segment and return results"""
    try:
        model = YOLO('yolov8n.pt')
        
        cap = cv2.VideoCapture(video_path)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        
        line_y = int(height * line_position)
        tracker = VehicleTracker(max_disappeared=fps, max_distance=150)
        class_counts = defaultdict(int)
        
        frame_count = 0
        
        # Output video path
        output_path = tempfile.NamedTemporaryFile(delete=False, suffix='.mp4').name
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_path, fourcc, fps//skip_frames, (width, height))
        
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
            
            frame_count += 1
            
            if frame_count % skip_frames != 0:
                continue
            
            results = model(frame, conf=confidence, verbose=False)
            
            detections = []
            for r in results:
                boxes = r.boxes
                for box in boxes:
                    cls = int(box.cls[0])
                    if cls in [2, 3, 5, 7]:
                        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                        centroid = ((x1 + x2) / 2, (y1 + y2) / 2)
                        bbox_area = (x2 - x1) * (y2 - y1)
                        fhwa_class = map_to_fhwa(cls, bbox_area)
                        detections.append((centroid, fhwa_class))
                        
                        cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)
                        cv2.circle(frame, (int(centroid[0]), int(centroid[1])), 4, (0, 0, 255), -1)
                        cv2.putText(frame, f"Class {fhwa_class}", (int(x1), int(y1)-10),
                                  cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
            
            newly_counted = tracker.update(detections, line_y)
            for fhwa_class in newly_counted:
                class_counts[fhwa_class] += 1
            
            cv2.line(frame, (0, line_y), (width, line_y), (0, 0, 255), 3)
            cv2.putText(frame, f"SEGMENT {segment_id}", (10, line_y - 10),
                      cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
            
            total_count = sum(class_counts.values())
            cv2.putText(frame, f"Total: {total_count}", (10, 30),
                      cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
            
            out.write(frame)
            
            # Update progress
            progress = frame_count / total_frames
            progress_dict[f'segment_{segment_id}'] = progress
        
        cap.release()
        out.release()
        
        # Return results
        result_queue.put({
            'segment_id': segment_id,
            'class_counts': dict(class_counts),
            'output_path': output_path
        })
        
    except Exception as e:
        result_queue.put({
            'segment_id': segment_id,
            'error': str(e)
        })

# Merge two videos using ffmpeg
def merge_videos(video1_path, video2_path, output_path):
    """Merge two videos using ffmpeg"""
    try:
        # Create concat file
        concat_file = tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.txt')
        concat_file.write(f"file '{video1_path}'\n")
        concat_file.write(f"file '{video2_path}'\n")
        concat_file.close()
        
        # Merge videos
        cmd = [
            'ffmpeg', '-f', 'concat', '-safe', '0',
            '-i', concat_file.name,
            '-c', 'copy',
            '-y', output_path
        ]
        
        subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        
        os.unlink(concat_file.name)
        return True
    except Exception as e:
        st.error(f"Error merging videos: {e}")
        return False

# File uploader with 10GB limit
uploaded_file = st.file_uploader("üìÅ Upload Video File (Max 10GB)", 
                                  type=['mp4', 'avi', 'mov', 'mkv'],
                                  accept_multiple_files=False)

if uploaded_file is not None:
    tfile = tempfile.NamedTemporaryFile(delete=False, suffix='.mp4')
    tfile.write(uploaded_file.read())
    video_path = tfile.name
    
    st.video(video_path)
    
    # Check video duration
    cap_temp = cv2.VideoCapture(video_path)
    total_frames_temp = int(cap_temp.get(cv2.CAP_PROP_FRAME_COUNT))
    fps_temp = int(cap_temp.get(cv2.CAP_PROP_FPS))
    video_duration_temp = total_frames_temp / fps_temp if fps_temp > 0 else 0
    cap_temp.release()
    
    is_long_video = video_duration_temp > 7200  # 2 hours
    
    if is_long_video and enable_parallel:
        st.info(f"üöÄ Video is {format_time(video_duration_temp)} - Parallel processing will be used (2x faster!)")
    else:
        st.info(f"üìä Video duration: {format_time(video_duration_temp)}")
    
    if st.button("‚ñ∂Ô∏è Start Processing", type="primary"):
        try:
            st.session_state.processed = False
            start_time = time.time()
            
            # Check if we should use parallel processing
            if is_long_video and enable_parallel:
                st.info("üîÑ Splitting video into 2 segments for parallel processing...")
                
                # Split video
                segment1_path = tempfile.NamedTemporaryFile(delete=False, suffix='.mp4').name
                segment2_path = tempfile.NamedTemporaryFile(delete=False, suffix='.mp4').name
                
                success, duration = split_video(video_path, segment1_path, segment2_path)
                
                if not success:
                    st.error("Failed to split video. Using single-threaded processing.")
                    raise Exception("Video splitting failed")
                
                st.success("‚úÖ Video split complete! Starting parallel processing...")
                
                # Create shared progress dictionary
                manager = Manager()
                progress_dict = manager.dict()
                progress_dict['segment_1'] = 0.0
                progress_dict['segment_2'] = 0.0
                
                # Create result queue
                result_queue = Queue()
                
                # Start two processes
                with st.spinner("üîÑ Loading YOLO models for both segments..."):
                    p1 = Process(target=process_video_segment, 
                                args=(segment1_path, confidence, skip_frames, line_position, 
                                      1, result_queue, progress_dict))
                    p2 = Process(target=process_video_segment, 
                                args=(segment2_path, confidence, skip_frames, line_position, 
                                      2, result_queue, progress_dict))
                    
                    p1.start()
                    p2.start()
                
                # Monitor progress
                progress_bar1 = st.progress(0)
                progress_bar2 = st.progress(0)
                status_text = st.empty()
                
                col1, col2 = st.columns(2)
                with col1:
                    st.write("üé¨ Segment 1 (First Half)")
                with col2:
                    st.write("üé¨ Segment 2 (Second Half)")
                
                while p1.is_alive() or p2.is_alive():
                    progress_bar1.progress(progress_dict.get('segment_1', 0))
                    progress_bar2.progress(progress_dict.get('segment_2', 0))
                    
                    elapsed = time.time() - start_time
                    status_text.text(f"‚è±Ô∏è Processing both segments... Elapsed: {format_time(elapsed)}")
                    time.sleep(0.5)
                
                p1.join()
                p2.join()
                
                progress_bar1.progress(1.0)
                progress_bar2.progress(1.0)
                
                # Get results
                results = []
                while not result_queue.empty():
                    results.append(result_queue.get())
                
                if len(results) != 2:
                    raise Exception("Failed to process both segments")
                
                # Sort by segment_id
                results.sort(key=lambda x: x['segment_id'])
                
                # Merge class counts
                class_counts = defaultdict(int)
                for result in results:
                    if 'error' in result:
                        raise Exception(f"Segment {result['segment_id']} error: {result['error']}")
                    for cls, count in result['class_counts'].items():
                        class_counts[cls] += count
                
                st.info("üîÑ Merging processed video segments...")
                
                # Merge videos
                output_path = tempfile.NamedTemporaryFile(delete=False, suffix='.mp4').name
                merge_success = merge_videos(results[0]['output_path'], 
                                            results[1]['output_path'], 
                                            output_path)
                
                if not merge_success:
                    st.warning("‚ö†Ô∏è Video merge failed, using first segment only")
                    output_path = results[0]['output_path']
                
                # Cleanup
                os.unlink(segment1_path)
                os.unlink(segment2_path)
                if merge_success:
                    os.unlink(results[0]['output_path'])
                    os.unlink(results[1]['output_path'])
                
            else:
                # Single-threaded processing (original code)
                with st.spinner("üîÑ Loading YOLOv8 model..."):
                    model = YOLO('yolov8n.pt')
                
                cap = cv2.VideoCapture(video_path)
                total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
                fps = int(cap.get(cv2.CAP_PROP_FPS))
                width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
                height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
                
                video_duration_seconds = total_frames / fps if fps > 0 else 0
                
                line_y = int(height * line_position)
                tracker = VehicleTracker(max_disappeared=fps, max_distance=150)
                class_counts = defaultdict(int)
                
                progress_bar = st.progress(0)
                status_text = st.empty()
                time_text = st.empty()
                frame_placeholder = st.empty()
                
                frame_count = 0
                processed_frames = 0
                
                output_path = tempfile.NamedTemporaryFile(delete=False, suffix='.mp4').name
                fourcc = cv2.VideoWriter_fourcc(*'mp4v')
                out = cv2.VideoWriter(output_path, fourcc, fps//skip_frames, (width, height))
                
                while cap.isOpened():
                    ret, frame = cap.read()
                    if not ret:
                        break
                    
                    frame_count += 1
                    
                    if frame_count % skip_frames != 0:
                        continue
                    
                    processed_frames += 1
                    
                    results = model(frame, conf=confidence, verbose=False)
                    
                    detections = []
                    for r in results:
                        boxes = r.boxes
                        for box in boxes:
                            cls = int(box.cls[0])
                            if cls in [2, 3, 5, 7]:
                                x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                                centroid = ((x1 + x2) / 2, (y1 + y2) / 2)
                                bbox_area = (x2 - x1) * (y2 - y1)
                                fhwa_class = map_to_fhwa(cls, bbox_area)
                                detections.append((centroid, fhwa_class))
                                
                                cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)
                                cv2.circle(frame, (int(centroid[0]), int(centroid[1])), 4, (0, 0, 255), -1)
                                cv2.putText(frame, f"Class {fhwa_class}", (int(x1), int(y1)-10),
                                          cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
                    
                    newly_counted = tracker.update(detections, line_y)
                    for fhwa_class in newly_counted:
                        class_counts[fhwa_class] += 1
                    
                    cv2.line(frame, (0, line_y), (width, line_y), (0, 0, 255), 3)
                    cv2.putText(frame, "COUNTING LINE", (10, line_y - 10),
                              cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
                    
                    y_offset = 30
                    total_count = sum(class_counts.values())
                    cv2.putText(frame, f"Total: {total_count}", (10, y_offset),
                              cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
                    
                    for cls, count in sorted(class_counts.items()):
                        if count > 0:
                            y_offset += 35
                            cv2.putText(frame, f"Class {cls}: {count}", (10, y_offset),
                                      cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)
                    
                    out.write(frame)
                    
                    progress = frame_count / total_frames
                    progress_bar.progress(progress)
                    
                    elapsed_time = time.time() - start_time
                    if progress > 0:
                        estimated_total = elapsed_time / progress
                        remaining_time = estimated_total - elapsed_time
                        time_text.text(f"‚è±Ô∏è Elapsed: {format_time(elapsed_time)} | Estimated remaining: {format_time(remaining_time)}")
                    
                    status_text.text(f"Processing: {frame_count}/{total_frames} frames | Detected: {total_count} vehicles")
                    
                    if processed_frames % 30 == 0:
                        frame_placeholder.image(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB), 
                                              channels="RGB", use_container_width=True)
                
                cap.release()
                out.release()
                video_duration_seconds = video_duration_temp
            
            # End timing
            end_time = time.time()
            processing_time = end_time - start_time
            
            total = sum(class_counts.values())
            
            # Store in session state
            st.session_state.processed = True
            st.session_state.output_video_path = output_path
            st.session_state.class_counts = dict(class_counts)
            st.session_state.total_count = total
            st.session_state.processing_time = processing_time
            st.session_state.video_duration = video_duration_temp
            
            results_data = []
            for cls in range(1, 14):
                count = class_counts.get(cls, 0)
                results_data.append({
                    'FHWA Class': cls,
                    'Vehicle Type': fhwa_classes[cls],
                    'Count': count,
                    'Percentage': f"{(count/total*100):.1f}%" if total > 0 else "0%"
                })
            
            st.session_state.results_df = pd.DataFrame(results_data)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            csv_data = {'Time Slot': [datetime.now().strftime("%I:%M %P")]}
            
            for i in range(1, 14):
                csv_data[f'Class {i}'] = [class_counts.get(i, 0)]
            
            csv_data['Total'] = [total]
            csv_df = pd.DataFrame(csv_data)
            st.session_state.csv_data = csv_df.to_csv(index=False)
            st.session_state.csv_filename = f"vehicle_counts_{timestamp}.csv"
            st.session_state.video_filename = f"processed_{uploaded_file.name}"
            
            st.success(f"‚úÖ Processing Complete! Total vehicles counted: {total}")
            st.balloons()
            st.rerun()
            
        except Exception as e:
            st.error(f"‚ùå Error: {str(e)}")
            st.exception(e)
    
    if not st.session_state.processed:
        os.unlink(video_path)

# Display results if processed (PERSISTENT)
if st.session_state.processed:
    processing_speed = (st.session_state.video_duration / st.session_state.processing_time) if st.session_state.processing_time > 0 else 0
    
    st.markdown(f"""
        <div style='background-color: #e8f4f8; padding: 15px; border-radius: 10px; margin: 20px 0; border-left: 5px solid #0066cc;'>
            <h3 style='color: #0066cc; margin: 0;'>‚è±Ô∏è Processing Time</h3>
            <p style='font-size: 1.3rem; font-weight: bold; color: #004080; margin: 10px 0 5px 0;'>
                {format_time(st.session_state.processing_time)}
            </p>
            <p style='color: #666; margin: 0; font-size: 0.9rem;'>
                Video Duration: {format_time(st.session_state.video_duration)} | 
                Processing Speed: {processing_speed:.1f}x faster than real-time
            </p>
        </div>
    """, unsafe_allow_html=True)
    
    st.subheader("‚¨áÔ∏è Download Results")
    
    col1, col2 = st.columns(2)
    
    with col1:
        if st.session_state.output_video_path and os.path.exists(st.session_state.output_video_path):
            with open(st.session_state.output_video_path, 'rb') as f:
                st.download_button(
                    label="üì• Download Processed Video",
                    data=f,
                    file_name=st.session_state.video_filename,
                    mime="video/mp4",
                    key="download_video"
                )
    
    with col2:
        if st.session_state.csv_data:
            st.download_button(
                label="üì• Download CSV Report",
                data=st.session_state.csv_data,
                file_name=st.session_state.csv_filename,
                mime="text/csv",
                key="download_csv"
            )
    
    st.subheader("üìä Vehicle Count Summary")
    col1, col2 = st.columns(2)
    
    with col1:
        st.metric("Total Vehicles", st.session_state.total_count)
        st.dataframe(st.session_state.results_df, use_container_width=True)
    
    with col2:
        st.subheader("üìà Distribution")
        chart_data = st.session_state.results_df[st.session_state.results_df['Count'] > 0][['Vehicle Type', 'Count']]
        if not chart_data.empty:
            st.bar_chart(chart_data.set_index('Vehicle Type'))
    
    st.subheader("üìÑ CSV Preview")
    csv_preview_df = pd.read_csv(pd.io.common.StringIO(st.session_state.csv_data))
    st.dataframe(csv_preview_df)
    
    if st.button("üîÑ Process Another Video"):
        if st.session_state.output_video_path and os.path.exists(st.session_state.output_video_path):
            os.unlink(st.session_state.output_video_path)
        
        st.session_state.processed = False
        st.session_state.output_video_path = None
        st.session_state.csv_data = None
        st.session_state.results_df = None
        st.session_state.class_counts = None
        st.session_state.total_count = 0
        st.session_state.processing_time = 0
        st.session_state.video_duration = 0
        st.rerun()

elif uploaded_file is None:
    st.info("üëÜ Please upload a video file to begin")
    st.markdown("""
    ### üìã Instructions:
    1. Upload a traffic video (MP4, AVI, MOV, MKV) - Max 10GB
    2. Enable "Parallel Processing" in sidebar for videos >2 hours
    3. Click "Start Processing" to analyze
    4. Download results and CSV report
    
    ### üöÄ **Parallel Processing Features:**
    - ‚úÖ **Auto-split** - Videos >2 hours split into 2 segments
    - ‚úÖ **Simultaneous processing** - Both segments processed at same time
    - ‚úÖ **~2x speedup** - On multi-core CPUs
    - ‚úÖ **Single output** - Merged video + combined CSV
    - ‚úÖ **Progress tracking** - See both segments in real-time
    
    ### üìä **Performance Comparison:**
    
    | Video Length | Normal | Parallel | Time Saved |
    |--------------|--------|----------|------------|
    | 2 hours | 1 hour | 30 min | 30 min |
    | 4 hours | 2 hours | 1 hour | 1 hour |
    | 8 hours | 4 hours | 2 hours | 2 hours |
    
    ### üéØ **Features:**
    - ‚úÖ No double counting
    - ‚úÖ FHWA 13-class classification
    - ‚úÖ Real-time progress tracking
    - ‚úÖ CSV export with combined counts
    - ‚úÖ Merged video output
    """)
