import streamlit as st
import cv2
from ultralytics import YOLO
import numpy as np
from collections import defaultdict
import pandas as pd
from datetime import datetime
import tempfile
import os
import time
import torch

# Page configuration
st.set_page_config(page_title="Vehicle Counter", layout="wide")

# Title with link to Fiverr
st.markdown("""
    <h1 style='text-align: center;'>
        üöó FHWA Vehicle Counter by 
        <a href='https://fiverr.com/naseem733' target='_blank' style='color: #1DBF73; text-decoration: none;'>
            Naseem Awan
        </a>
    </h1>
""", unsafe_allow_html=True)

# Initialize session state for persistence
if 'processed' not in st.session_state:
    st.session_state.processed = False
if 'output_video_path' not in st.session_state:
    st.session_state.output_video_path = None
if 'csv_data' not in st.session_state:
    st.session_state.csv_data = None
if 'results_df' not in st.session_state:
    st.session_state.results_df = None
if 'class_counts' not in st.session_state:
    st.session_state.class_counts = None
if 'total_count' not in st.session_state:
    st.session_state.total_count = 0
if 'processing_time' not in st.session_state:
    st.session_state.processing_time = 0
if 'video_duration' not in st.session_state:
    st.session_state.video_duration = 0

# Sidebar configuration
st.sidebar.header("‚öôÔ∏è Configuration")
confidence = st.sidebar.slider("Detection Confidence", 0.1, 1.0, 0.25, 0.05)
skip_frames = st.sidebar.slider("Skip Frames (Speed)", 1, 10, 2)
line_position = st.sidebar.slider("Counting Line Position", 0.0, 1.0, 0.5, 0.05)

# Check if GPU is available
gpu_available = torch.cuda.is_available()

# Advanced settings
with st.sidebar.expander("üöÄ Advanced Speed Settings"):
    if gpu_available:
        use_gpu = st.checkbox("Use GPU Acceleration", value=True, 
                             help="Use CUDA GPU for faster processing")
        use_half_precision = st.checkbox("Use Half Precision (FP16)", value=True, 
                                         help="2x faster with GPU (requires GPU)")
    else:
        st.warning("‚ö†Ô∏è No GPU detected - using CPU optimizations")
        use_gpu = False
        use_half_precision = False
    
    use_smaller_model = st.checkbox("Use Nano Model (Faster)", value=True,
                                    help="YOLOv8n is faster than larger models")
    reduce_resolution = st.checkbox("Reduce Video Resolution", value=False,
                                    help="Process at lower resolution (faster but less accurate)")
    resolution_scale = st.slider("Resolution Scale", 0.5, 1.0, 0.75, 0.05) if reduce_resolution else 1.0
    
    optimize_tracking = st.checkbox("Optimize Tracking", value=True,
                                   help="Reduce tracking overhead for speed")

# FHWA Vehicle Classes
st.sidebar.header("üìä FHWA Vehicle Classes")
fhwa_classes = {
    1: "Motorcycle",
    2: "Passenger Car",
    3: "Pickup/Van",
    4: "Bus",
    5: "Single Unit Truck (2-axle)",
    6: "Single Unit Truck (3-axle)",
    7: "Single Unit Truck (4+ axle)",
    8: "Single Trailer Truck (3-4 axle)",
    9: "Single Trailer Truck (5-axle)",
    10: "Single Trailer Truck (6+ axle)",
    11: "Multi-Trailer Truck (5 axle)",
    12: "Multi-Trailer Truck (6 axle)",
    13: "Multi-Trailer Truck (7+ axle)"
}

for cls, name in fhwa_classes.items():
    st.sidebar.text(f"Class {cls}: {name}")

# Helper function to format time
def format_time(seconds):
    if seconds < 60:
        return f"{seconds:.1f} seconds"
    elif seconds < 3600:
        minutes = seconds / 60
        return f"{minutes:.1f} minutes"
    else:
        hours = seconds / 3600
        minutes = (seconds % 3600) / 60
        return f"{hours:.1f} hours ({minutes:.0f} minutes)"

# YOLO to FHWA mapping
def map_to_fhwa(yolo_class, bbox_area):
    """Map YOLO class to FHWA vehicle class"""
    if yolo_class == 3:  # motorcycle
        return 1
    elif yolo_class == 2:  # car
        if bbox_area < 5000:
            return 2  # Passenger car
        else:
            return 3  # Pickup/Van
    elif yolo_class == 5:  # bus
        return 4
    elif yolo_class == 7:  # truck
        if bbox_area < 8000:
            return 5  # Small truck
        elif bbox_area < 15000:
            return 8  # Medium truck
        else:
            return 9  # Large truck
    return 2  # Default to passenger car

# Vehicle tracker
class VehicleTracker:
    def __init__(self, max_disappeared=30, max_distance=100, optimize=False):
        self.next_object_id = 0
        self.objects = {}
        self.disappeared = {}
        self.counted = set()
        self.max_disappeared = max_disappeared
        self.max_distance = max_distance
        self.optimize = optimize
        
    def register(self, centroid, fhwa_class):
        self.objects[self.next_object_id] = {
            'centroid': centroid,
            'class': fhwa_class,
            'crossed': False
        }
        self.disappeared[self.next_object_id] = 0
        self.next_object_id += 1
        
    def deregister(self, object_id):
        del self.objects[object_id]
        del self.disappeared[object_id]
        
    def update(self, detections, line_y):
        newly_counted = []
        
        if len(detections) == 0:
            for object_id in list(self.disappeared.keys()):
                self.disappeared[object_id] += 1
                if self.disappeared[object_id] > self.max_disappeared:
                    self.deregister(object_id)
            return newly_counted
        
        input_centroids = np.array([d[0] for d in detections])
        input_classes = [d[1] for d in detections]
        
        if len(self.objects) == 0:
            for i, (centroid, fhwa_class) in enumerate(detections):
                self.register(centroid, fhwa_class)
        else:
            object_ids = list(self.objects.keys())
            object_centroids = np.array([self.objects[oid]['centroid'] for oid in object_ids])
            
            # Optimized distance calculation
            if self.optimize and len(object_centroids) > 10:
                # Use faster approximate matching for many objects
                D = np.abs(object_centroids[:, np.newaxis, 0] - input_centroids[:, 0]) + \
                    np.abs(object_centroids[:, np.newaxis, 1] - input_centroids[:, 1])
            else:
                D = np.linalg.norm(object_centroids[:, np.newaxis] - input_centroids, axis=2)
            
            rows = D.min(axis=1).argsort()
            cols = D.argmin(axis=1)[rows]
            
            used_rows = set()
            used_cols = set()
            
            for (row, col) in zip(rows, cols):
                if row in used_rows or col in used_cols:
                    continue
                    
                if D[row, col] > self.max_distance:
                    continue
                    
                object_id = object_ids[row]
                old_centroid = self.objects[object_id]['centroid']
                new_centroid = input_centroids[col]
                
                self.objects[object_id]['centroid'] = new_centroid
                self.disappeared[object_id] = 0
                
                if (object_id not in self.counted and 
                    not self.objects[object_id]['crossed'] and
                    old_centroid[1] < line_y <= new_centroid[1]):
                    
                    self.objects[object_id]['crossed'] = True
                    self.counted.add(object_id)
                    newly_counted.append(self.objects[object_id]['class'])
                
                used_rows.add(row)
                used_cols.add(col)
            
            unused_rows = set(range(D.shape[0])) - used_rows
            for row in unused_rows:
                object_id = object_ids[row]
                self.disappeared[object_id] += 1
                if self.disappeared[object_id] > self.max_disappeared:
                    self.deregister(object_id)
            
            unused_cols = set(range(D.shape[1])) - used_cols
            for col in unused_cols:
                self.register(input_centroids[col], input_classes[col])
        
        return newly_counted

# File uploader with 10GB limit
uploaded_file = st.file_uploader("üìÅ Upload Video File (Max 10GB)", 
                                  type=['mp4', 'avi', 'mov', 'mkv'],
                                  accept_multiple_files=False)

if uploaded_file is not None:
    tfile = tempfile.NamedTemporaryFile(delete=False, suffix='.mp4')
    tfile.write(uploaded_file.read())
    video_path = tfile.name
    
    st.video(video_path)
    
    # Display estimated processing time
    cap_temp = cv2.VideoCapture(video_path)
    total_frames_temp = int(cap_temp.get(cv2.CAP_PROP_FRAME_COUNT))
    fps_temp = int(cap_temp.get(cv2.CAP_PROP_FPS))
    video_duration_temp = total_frames_temp / fps_temp if fps_temp > 0 else 0
    cap_temp.release()
    
    # Estimate processing time based on settings
    speed_multiplier = 1.0
    if use_gpu and use_half_precision:
        speed_multiplier *= 3.0
    elif use_gpu:
        speed_multiplier *= 2.0
    if reduce_resolution:
        speed_multiplier *= (1.0 / (resolution_scale ** 2))
    if optimize_tracking:
        speed_multiplier *= 1.2
    
    estimated_time = (video_duration_temp / speed_multiplier) / skip_frames
    
    st.info(f"üìä Video Info: {format_time(video_duration_temp)} | Estimated Processing Time: ~{format_time(estimated_time)}")
    
    if st.button("‚ñ∂Ô∏è Start Processing", type="primary"):
        try:
            st.session_state.processed = False
            
            # Start timing
            start_time = time.time()
            
            with st.spinner("üîÑ Loading YOLOv8 model with optimizations..."):
                model_name = 'yolov8n.pt' if use_smaller_model else 'yolov8s.pt'
                model = YOLO(model_name)
                
                # Set device
                device = 'cuda:0' if use_gpu else 'cpu'
                
                # Enable half precision only if GPU is available
                if use_gpu and use_half_precision:
                    try:
                        model.model.half()
                        st.success("‚úÖ GPU + Half precision (FP16) enabled - 3x faster!")
                    except Exception as e:
                        st.warning(f"‚ö†Ô∏è Could not enable FP16: {e}")
                        use_half_precision = False
                elif use_gpu:
                    st.success("‚úÖ GPU acceleration enabled - 2x faster!")
                else:
                    st.info("‚ÑπÔ∏è Using CPU with optimizations")
            
            cap = cv2.VideoCapture(video_path)
            
            # Optimize video capture
            cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
            
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            fps = int(cap.get(cv2.CAP_PROP_FPS))
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            
            # Reduce resolution if enabled
            process_width = int(width * resolution_scale) if reduce_resolution else width
            process_height = int(height * resolution_scale) if reduce_resolution else height
            
            # Calculate estimated video duration
            video_duration_seconds = total_frames / fps if fps > 0 else 0
            
            line_y = int(process_height * line_position)
            tracker = VehicleTracker(max_disappeared=fps, max_distance=150, optimize=optimize_tracking)
            class_counts = defaultdict(int)
            
            progress_bar = st.progress(0)
            status_text = st.empty()
            time_text = st.empty()
            frame_placeholder = st.empty()
            
            frame_count = 0
            processed_frames = 0
            
            output_path = tempfile.NamedTemporaryFile(delete=False, suffix='.mp4').name
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(output_path, fourcc, fps//skip_frames, (width, height))
            
            while cap.isOpened():
                ret, frame = cap.read()
                if not ret:
                    break
                
                frame_count += 1
                
                if frame_count % skip_frames != 0:
                    continue
                
                processed_frames += 1
                
                # Resize for processing if needed
                if reduce_resolution:
                    process_frame = cv2.resize(frame, (process_width, process_height), 
                                              interpolation=cv2.INTER_LINEAR)
                else:
                    process_frame = frame
                
                # Run inference
                results = model(process_frame, conf=confidence, verbose=False, 
                              device=device, half=use_half_precision if use_gpu else False)
                
                detections = []
                for r in results:
                    boxes = r.boxes
                    for box in boxes:
                        cls = int(box.cls[0])
                        if cls in [2, 3, 5, 7]:
                            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                            
                            # Scale back to original if resolution was reduced
                            if reduce_resolution:
                                scale = 1.0 / resolution_scale
                                x1, y1, x2, y2 = x1*scale, y1*scale, x2*scale, y2*scale
                            
                            centroid = ((x1 + x2) / 2, (y1 + y2) / 2)
                            bbox_area = (x2 - x1) * (y2 - y1)
                            fhwa_class = map_to_fhwa(cls, bbox_area)
                            detections.append((centroid, fhwa_class))
                            
                            cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)
                            cv2.circle(frame, (int(centroid[0]), int(centroid[1])), 4, (0, 0, 255), -1)
                            cv2.putText(frame, f"Class {fhwa_class}", (int(x1), int(y1)-10),
                                      cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
                
                # Scale line_y back to original resolution for tracking
                track_line_y = line_y if not reduce_resolution else int(line_y / resolution_scale)
                newly_counted = tracker.update(detections, track_line_y)
                for fhwa_class in newly_counted:
                    class_counts[fhwa_class] += 1
                
                # Draw on original resolution frame
                draw_line_y = int(height * line_position)
                cv2.line(frame, (0, draw_line_y), (width, draw_line_y), (0, 0, 255), 3)
                cv2.putText(frame, "COUNTING LINE", (10, draw_line_y - 10),
                          cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
                
                y_offset = 30
                total_count = sum(class_counts.values())
                cv2.putText(frame, f"Total: {total_count}", (10, y_offset),
                          cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
                
                for cls, count in sorted(class_counts.items()):
                    if count > 0:
                        y_offset += 35
                        cv2.putText(frame, f"Class {cls}: {count}", (10, y_offset),
                                  cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)
                
                out.write(frame)
                
                progress = frame_count / total_frames
                progress_bar.progress(progress)
                
                # Calculate elapsed and estimated time
                elapsed_time = time.time() - start_time
                if progress > 0:
                    estimated_total = elapsed_time / progress
                    remaining_time = estimated_total - elapsed_time
                    time_text.text(f"‚è±Ô∏è Elapsed: {format_time(elapsed_time)} | Estimated remaining: {format_time(remaining_time)}")
                
                total_count = sum(class_counts.values())
                status_text.text(f"Processing: {frame_count}/{total_frames} frames | Detected: {total_count} vehicles")
                
                if processed_frames % 30 == 0:
                    frame_placeholder.image(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB), 
                                          channels="RGB", use_container_width=True)
            
            cap.release()
            out.release()
            
            # End timing
            end_time = time.time()
            processing_time = end_time - start_time
            
            total = sum(class_counts.values())
            
            # Store in session state
            st.session_state.processed = True
            st.session_state.output_video_path = output_path
            st.session_state.class_counts = dict(class_counts)
            st.session_state.total_count = total
            st.session_state.processing_time = processing_time
            st.session_state.video_duration = video_duration_seconds
            
            results_data = []
            for cls in range(1, 14):
                count = class_counts.get(cls, 0)
                results_data.append({
                    'FHWA Class': cls,
                    'Vehicle Type': fhwa_classes[cls],
                    'Count': count,
                    'Percentage': f"{(count/total*100):.1f}%" if total > 0 else "0%"
                })
            
            st.session_state.results_df = pd.DataFrame(results_data)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            csv_data = {'Time Slot': [datetime.now().strftime("%I:%M %P")]}
            
            for i in range(1, 14):
                csv_data[f'Class {i}'] = [class_counts.get(i, 0)]
            
            csv_data['Total'] = [total]
            csv_df = pd.DataFrame(csv_data)
            st.session_state.csv_data = csv_df.to_csv(index=False)
            st.session_state.csv_filename = f"vehicle_counts_{timestamp}.csv"
            st.session_state.video_filename = f"processed_{uploaded_file.name}"
            
            st.success(f"‚úÖ Processing Complete! Total vehicles counted: {total}")
            st.balloons()
            st.rerun()
            
        except Exception as e:
            st.error(f"‚ùå Error: {str(e)}")
            st.exception(e)
    
    if not st.session_state.processed:
        os.unlink(video_path)

# Display results if processed (PERSISTENT)
if st.session_state.processed:
    # Display processing time prominently
    processing_speed = (st.session_state.video_duration / st.session_state.processing_time) if st.session_state.processing_time > 0 else 0
    
    st.markdown(f"""
        <div style='background-color: #e8f4f8; padding: 15px; border-radius: 10px; margin: 20px 0; border-left: 5px solid #0066cc;'>
            <h3 style='color: #0066cc; margin: 0;'>‚è±Ô∏è Processing Time</h3>
            <p style='font-size: 1.3rem; font-weight: bold; color: #004080; margin: 10px 0 5px 0;'>
                {format_time(st.session_state.processing_time)}
            </p>
            <p style='color: #666; margin: 0; font-size: 0.9rem;'>
                Video Duration: {format_time(st.session_state.video_duration)} | 
                Processing Speed: {processing_speed:.1f}x faster than real-time
            </p>
        </div>
    """, unsafe_allow_html=True)
    
    st.subheader("‚¨áÔ∏è Download Results")
    
    col1, col2 = st.columns(2)
    
    with col1:
        if st.session_state.output_video_path and os.path.exists(st.session_state.output_video_path):
            with open(st.session_state.output_video_path, 'rb') as f:
                st.download_button(
                    label="üì• Download Processed Video",
                    data=f,
                    file_name=st.session_state.video_filename,
                    mime="video/mp4",
                    key="download_video"
                )
    
    with col2:
        if st.session_state.csv_data:
            st.download_button(
                label="üì• Download CSV Report",
                data=st.session_state.csv_data,
                file_name=st.session_state.csv_filename,
                mime="text/csv",
                key="download_csv"
            )
    
    st.subheader("üìä Vehicle Count Summary")
    col1, col2 = st.columns(2)
    
    with col1:
        st.metric("Total Vehicles", st.session_state.total_count)
        st.dataframe(st.session_state.results_df, use_container_width=True)
    
    with col2:
        st.subheader("üìà Distribution")
        chart_data = st.session_state.results_df[st.session_state.results_df['Count'] > 0][['Vehicle Type', 'Count']]
        if not chart_data.empty:
            st.bar_chart(chart_data.set_index('Vehicle Type'))
    
    st.subheader("üìÑ CSV Preview")
    csv_preview_df = pd.read_csv(pd.io.common.StringIO(st.session_state.csv_data))
    st.dataframe(csv_preview_df)
    
    if st.button("üîÑ Process Another Video"):
        if st.session_state.output_video_path and os.path.exists(st.session_state.output_video_path):
            os.unlink(st.session_state.output_video_path)
        
        st.session_state.processed = False
        st.session_state.output_video_path = None
        st.session_state.csv_data = None
        st.session_state.results_df = None
        st.session_state.class_counts = None
        st.session_state.total_count = 0
        st.session_state.processing_time = 0
        st.session_state.video_duration = 0
        st.rerun()

elif uploaded_file is None:
    st.info("üëÜ Please upload a video file to begin")
    st.markdown("""
    ### üìã Instructions:
    1. Upload a traffic video (MP4, AVI, MOV, MKV) - Max 10GB
    2. Adjust detection confidence and counting line position
    3. **Configure speed optimizations** in "Advanced Speed Settings"
    4. Click "Start Processing" to analyze
    5. Download results and CSV report
    
    ### üéØ Features:
    - ‚úÖ **No double counting** - Advanced tracking prevents re-counting
    - ‚úÖ **FHWA classification** - Automatic vehicle type detection
    - ‚úÖ **Real-time progress** - See detection as it processes
    - ‚úÖ **Export results** - Download video and CSV reports
    - ‚úÖ **Large file support** - Up to 10GB video files
    - ‚úÖ **üöÄ CPU Optimized** - Fast processing even without GPU!
    
    ### ‚ö° Speed Optimization (CPU):
    - **Skip Frames = 2**: Best balance of speed and accuracy ‚úÖ
    - **Reduce Resolution (0.75)**: 30-40% faster processing
    - **Optimize Tracking**: Faster object tracking algorithm
    - **Nano Model**: Faster inference with good accuracy
    
    ### üéÆ With GPU (if available):
    - **GPU + FP16**: 3x faster than CPU
    - **GPU only**: 2x faster than CPU
    
    **For 8-hour video on CPU**: With all optimizations = ~2-3 hours processing time
    **For 8-hour video with GPU**: With FP16 = ~1-1.5 hours processing time üöÄ
    """)
